---
layout: post
comments: true
title: Exploring Transformers for both online and offline RL
author: Shruthi Srinarasi and Raunak Sinha (Team 28)
date: 2022-10-19
---


> One of the most prominent areas of Reinforcement Learning (RL) research is in gaming. This project will mainly compare different RL appraoches in gaming, especially Transformer RL. We intend to analyze different approaches in depth and discuss the benefits and drawbacks of various approaches. We also plan to briefly explore transfer learning in RL.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Main Content
This project will focus on implementing various RL methods for games. We will be experimenting on existing game environments provided by OpenAI Gym. The models we intend to analyze are:
* Simple DQN Network
* RNN-based DRQN Network
* Transformer-based DTQN Network

## Related Work
1. Boustati, A., Chockler, H., & McNamee, D. C. (2021). Transfer learning with causal counterfactual reasoning in Decision Transformers. arXiv. https://doi.org/10.48550/arXiv.2110.14355
2. Zheng, Q., Zhang, A., & Grover, A. (2022). Online Decision Transformer. arXiv. https://doi.org/10.48550/arXiv.2202.05607
Janner, M., Li, Q., & Levine, S. (2021). Offline Reinforcement Learning as One Big Sequence Modeling Problem. arXiv. https://doi.org/10.48550/arXiv.2106.02039
3. Paster, K., McIlraith, S., & Ba, J. (2022). You Can't Count on Luck: Why Decision Transformers Fail in Stochastic Environments. arXiv. https://doi.org/10.48550/arXiv.2205.15967
4. Wang, K., Zhao, H., Luo, X., Ren, K., Zhang, W., & Li, D. (2022). Bootstrapped Transformer for Offline Reinforcement Learning. arXiv. https://doi.org/10.48550/arXiv.2206.08569
5. Brandfonbrener, D., Bietti, A., Buckman, J., Laroche, R., & Bruna, J. (2022). When does return-conditioned supervised learning work for offline reinforcement learning?. arXiv. https://doi.org/10.48550/arXiv.2206.01079
6. Xu, M., Shen, Y., Zhang, S., Lu, Y., Zhao, D., Tenenbaum, J. B., & Gan, C. (2022). Prompting Decision Transformer for Few-Shot Policy Generalization. arXiv. https://doi.org/10.48550/arXiv.2206.13499
7. Villaflor, A.R., Huang, Z., Pande, S., Dolan, J.M. &amp; Schneider, J.. (2022). Addressing Optimism Bias in Sequence Modeling for Reinforcement Learning. <i>Proceedings of the 39th International Conference on Machine Learning</i>, in <i>Proceedings of Machine Learning Research</i> 162:22270-22283 Available from https://proceedings.mlr.press/v162/villaflor22a.html.
8. Upadhyay, U., Shah, N., Ravikanti, S., & Medhe, M. (2019). Transformer Based Reinforcement Learning For Games. arXiv. https://doi.org/10.48550/arXiv.1912.03918
9. Zhu, Z., Lin, K., Jain, A. K., & Zhou, J. (2020). Transfer Learning in Deep Reinforcement Learning: A Survey. arXiv. https://doi.org/10.48550/arXiv.2009.07888
10. Hausknecht, M., & Stone, P. (2015). Deep Recurrent Q-Learning for Partially Observable MDPs. arXiv. https://doi.org/10.48550/arXiv.1507.06527
11. Esslinger, K., Platt, R., & Amato, C. (2022). Deep Transformer Q-Networks for Partially Observable Reinforcement Learning. arXiv. https://doi.org/10.48550/arXiv.2206.01078

## Environments
* OpenAI Gym
* PyTorch
* Python3
* Anaconda Environment
* tensorboardx
* tensorflow

## November 11th Update
We are currently setting up the environment to run the experiments. We will be evaluating three different models for existing OpenAI Gym environments (CartPole, MountainCar). The three main models we will be evaluating are:
* Simple DQN Network
* RNN-based DRQN Network
* Transformer-based DTQN Network

### Deep Q Networks (DQN)
A DQN fundamentally consists of a mapping table, called the Q-table, which maps a state-action pair to a Q-value. The dimensions of the Q-table are (s,a) where s: state and a: action. The DQN has two main neural networks, the Q-network and Target network. It also consists of a component known an Experience Replay, which interacts with the environment to generate training data. (seen in Fig. 1.)

| ![RL_mid1](https://user-images.githubusercontent.com/65851937/201502949-0ffac8cf-fcb6-4484-ac34-05c84cf9ab53.png) |
|:--:| 
| *Figure 1. DQN Architecture* |

### Deep Recurrent Q-Learning Networks (DRQN)
A DRQN is designed by adding a Recurrent Neural Network (RNN) on top of a simple DQN. DQN. The architecture of DRQN augments DQN’s fully connected layer with a LSTM. The last L states are fed into a CNN to get intermediate outputs. These intermediate outputs are then fed to the RNN layer, which is used to predict the Q-value. (seen in Fig. 2.) 

| <img width="675" alt="Screenshot 2022-11-13 at 4 42 40 PM" src="https://user-images.githubusercontent.com/65851937/201553687-902440ce-78d0-4501-9ce6-d41e39926ef5.png"> |
|:--:| 
| *Figure 2. DRQN Architecture* |

### Deep Transformer Q-Networks (DTQN)
DTQN uses a transformer decoder structure which incorporates learned position encodings. The model training is done using Q-values generated for each timestep in the agent’s observation history. Specifically, given an agent’s observation history, a DTQN is trained to generate Q-values for each timestep in the history. Each observation in the history is embedded independently, and Q-values are generated for each observation sub-history. Only the last set of Q-values are used to select the next action, but the other Q-values can be utilized for training. (seen inf Fig. 3.)

| <img width="569" alt="Screenshot 2022-11-13 at 4 52 09 PM" src="https://user-images.githubusercontent.com/65851937/201554272-9e63cf75-02b6-49b9-a4c5-5c15f04df64a.png"> |
|:--:| 
| *Figure 3. DTQN Architecture* | 




<!-- ## Basic Syntax
### Image
Please create a folder with the name of your team id under `/assets/images/`, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/team00/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:

$$
\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}
$$

or you can write in-text formula $$y = wx + b$$.

### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/).

## Reference
Please make sure to cite properly in your work, for example:

[1] Dwibedi, Debidatta, et al. "Counting out time: Class agnostic video repetition counting in the wild." Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.   

[Peng, et al.] Peng, Zhenghao, et al. "Maybe you can also use other format for reference as you wish." Nature. 2022. 

---


## Data Rich and Physics Certain

| Experiment 					| Parameters  											| Results  								| Comments 							|
| :---       					|    :----:   											|     :---: 							|     ---: 							|
| **DL + Data**																																						|

| Predicting only velocity  	| Dataset size : 10000<br> Network : 2->5->5->1 <br> activation: ReLU	|  ~100% accurate	| Generalises well over various initial velocities |
| Predicting only displacement 	| Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable		| Better prediction for $u_0 \in dataset$, average prediction outside | 
| Predicting both $v_t, s_t$	| Dataset size : 10000<br> Network : 2->16->16->2 <br>	activation: tanh	|	Reasonable		| Better prediction for $u_0 \in dataset$, poor prediction outside |

-----

| **DL + Physics**																																			|
| Predicting both $v_t, s_t$, using Loss $L_{physics} = \|v_{predicted}^2-u_{initial}^2-2*g*s_{predicted}\|$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	~0% accuracy		| Expected result as no supervision of any kind is provided |
| Predicting both $v_t, s_t$, using Loss $L_{velocity+phy} = (v_{predicted}-v_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Prediction of $v_t$ is good. Was able to learn $s_t$ reasonably well without direct supervision |
| Predicting both $v_t, s_t$, using Loss $L_{supervised+phy} = (v_{predicted}-v_{actual})^2+(s_{predicted}-s_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Not a better result w.r.t direct supervision |


**Observations :** 
- Physics equations are certain in this case and are the best to use.
- Both DL, Hybrid(DL+Physics) methods performance are equivalent (actual accuracy/loss varies based on fine training, random dataset generation)

Re running the above experiments with Dataset size of 200(Data Starvation), yielded the following observations
- DL performance is comparable with 10000 dataset when trained on much mode epochs(5x)
- Hybrid(DL+Physics) without direct supervision on $s_t$ has comparable/better closeness than DL only method for limited epochs($\sim$300) training.




## Data Rich and Physics Uncertain

| Experiment 					| Parameters  											| Results  								| Comments 							|
| :---       					|    :----:   											|     :---: 							|     ---: 							|
| **DL + Data**																																						|\
| Predicting both $v_t, s_t$	| Dataset size : 10000<br> Network : 2->16->16->2 <br>	activation: tanh	|	Reasonable		| Better prediction for $u_0 \in dataset$, poor prediction outside |
| **DL + Physics**																																			|
| Predicting both $v_t, s_t$<br> using Loss $L_{physics} = \|v_{predicted}^2-u_{initial}^2-2*g*s_{predicted}\|$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	~0% accuracy		| Expected result as no supervision of any kind is provided |
| Predicting both $v_t, s_t$<br> using Loss $L_{velocity+phy} = (v_{predicted}-v_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Prediction of $v_t$ is good. Was able to learn $s_t$ reasonably well without direct supervision |
| Predicting both $v_t, s_t$<br> using Loss $L_{supervised+phy} = (v_{predicted}-v_{actual})^2+(s_{predicted}-s_{actual})^2+\gamma*(v_{predicted}^2-u_{initial}^2-2*g*s_{predicted})^2$ | Dataset size : 10000<br> Network : 2->16->16->1 <br>	activation: ReLU |	Reasonable	| Not a better result w.r.t direct supervision, but bettr than DL when $u0$ is out of dataset |


**Observations :** 
- Both DL, Hybrid(DL+Physics) methods performance are similar, Hybrid(DL+Physics) is better when $u0$ is out of dataset, DL is better for $u0$ in dataset.
- Physics equations are not certain in this case and the above methods are better to use than Physics.

## Data Starvation and Physics Uncertain
- Similar observations as in data rich -->


